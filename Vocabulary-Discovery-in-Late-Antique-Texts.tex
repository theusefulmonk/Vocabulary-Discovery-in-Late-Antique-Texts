% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
]{tufte-handout}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{255,255,255}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{\textbf{\colorbox[rgb]{0.97,0.90,0.90}{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.79,0.38,0.79}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.39,0.29,0.61}{\textbf{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.57,0.30,0.62}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.54,0.53,0.53}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.00,0.58,1.00}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.67,0.33,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.38,0.47,0.50}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{\underline{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.58,1.00}{\textbf{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.39,0.29,0.61}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{1.00,0.33,0.00}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.69,0.50,0.00}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.12,0.11,0.11}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.43,0.16}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.00,0.43,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{\colorbox[rgb]{0.88,0.91,0.97}{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.24,0.68,0.91}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{1.00,0.33,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.34,0.68}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.75,0.01,0.01}{#1}}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
%\usepackage[svgnames]{xcolor}
%\definecolor{codebackground}{RGB}{240, 240, 235}
%\definecolor{codebackground}{RGB}{117, 128, 124}
%\AtBeginDocument{\colorlet{defaultcolor}{.}}
%\definecolor{bg}{HTML}{282828} % from https://github.com/kevinsawicki/monokai
%\usepackage[outputdir=build]{minted}
%\setminted{style=monokai,bgcolor=bg}
%\setmintedinline{style=monokai,bgcolor=None}
%\definecolor{Text}{HTML}{F8F8F2}
%\AddToHook{cmd/mintinline/before}{\color{Text}}
%\AddToHook{cmd/mintinline/after}{}
    %\AtBeginEnvironment{minted}{\color{Text}}
\usepackage{pgfornament}
\usepackage{setspace}
\usepackage{microtype}
\usepackage{fontspec}
    \defaultfontfeatures{Numbers=OldStyle}
    \setmainfont{STIX Two Text}
\setmonofont{PragmataPro Mono Liga}
%\renewcommand{\footnote}[1]{\sidenote{#1}}
%\renewcommand{\familydefault}{\sfdefault}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Efficient Vocabulary Discovery in Late Antique Texts at the UNIX Command Line},
  pdfauthor={Andrew J. Hayes},
  colorlinks=true,
  linkcolor={teal},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Efficient Vocabulary Discovery in Late Antique Texts at the UNIX
Command Line}
\author{Andrew J. Hayes}
\date{May 23, 2025}

\begin{document}
\maketitle

\ifdefined\soulregister

\soulregister\MakeTextUppercase{1} \soulregister\MakeTextLowercase{1}
\soulregister\newlinetospace{1} \fi

\onehalfspacing

\section{Introduction}\label{introduction}

This presentation concerns a workflow for discovering vocabulary in
digitized late antique texts. I present this workflow first by
identifying the use cases for it and outlining the goals this
presentation hopes to achieve and those it does not. A conceptual
overview follows, with the aim of providing just enough background
knowledge to use the workflow. I present the workflow in overview and
interactively demonstrate its core steps. I conclude by highlighting the
workflow's limitations and imparting further resources that will enable
interested attendees to experiment with the workflow themselves.

The examples concern the author's area of expertise: which is the corpus
of the fourth century poet and theologian, Ephrem the Syrian. Most of
Ephrem's works were critically edited in the previous century by Edmund
Beck, for which he also provided high quality German
translations.\footnote{Sebastian Brock, {``Bibliographical Handouts by
  Brock''} (\url{https://syri.ac/brock}, 2025). See in particular
  ``St.~Ephrem: A Brief Guide to the Main Editions and Translations.''}
The demonstrations in this talk will use these German translations. But
the methods presented here can apply to any digitized text in LTR
unicode script.

\section{Use Cases}\label{use-cases}

The workflow demonstrated here is designed to address a particular
difficulty in patristic scholarship: searching ancient text(s) (whether
in the original or in translation) for which no born digital search tool
is available or accessible. Many collections of primary sources have
such tools. The \href{https://syriaccorpus.org/index.html}{Digital
Syriac Corpus} is an example of a native digital collection with robust
search tools. The \href{https://stephanus.tlg.uci.edu}{Thesaurus Linguae
Graecae} is another. Using such tools, if available, is ordinarily
preferrable to the approach described in this presentation.

But what to do if no such tool exists, is pay-walled, or is inaccessible
for some other reason? Many scholars maintain their own corpora of
scanned pdfs for private research use. Optical character recognition
(OCR) makes digital search of such texts possible. This presentation
shows how to search a local directory tree of OCR'ed pdf files of
primary sources in original or in translation in an efficient manner
using freely available tools in the UNIX programming environment. These
methods are imperfect, but nevertheless highly useful for assembling a
working vocabulary list as a starting point for careful reading and
research. Accompanying this presentation is a
\href{==insert\%20link==}{public repository on github} with sample
command-line recipes and instructions allowing any scholar to download
and experiment with them.

Note that even when a native digital search tool is available, it is
sometimes convenient and practical to employ the tools and techniques
presented here. The following are four such cases:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  One wants to search a large number of pdf files from disparate corpora
  at once.
\item
  One wants to search in multiple languages simultaneously.
\item
  One wants to automate or script complex queries for reproducibility
  and convenience.
\item
  One wants to produce a customized report of search results for use in
  another application.
\end{enumerate}

The examples in this talk involve searching Ephrem the Syrian's corpus,
mostly in the form of Edmund Beck's German translations, using scans
made for personal use from purchased physical volumes. The tools and
techniques are, however, applicable to any pdf containing LTR text in a
Roman or Greek alphabet, or any such script supported by a terminal
emulator.

\section{Three Core Steps (Goals and
Non-Goals)}\label{three-core-steps-goals-and-non-goals}

These workflows are complex. It is impossible to describe all their
parts fully within the scope of this presentation. Some of the parts
omitted from the main discussion are identified and augmented in the
Github repository with hints and suggestions for how one might best
accomplish the task. Nevertheless, the main business of this talk is to
discuss the three core steps in the workflow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Repaginate
\item
  Explore
\item
  Report
\end{enumerate}

\section{Background}\label{background}

\subsection{UNIX and the Command Line}\label{unix-and-the-command-line}

UNIX is a family of operating systems descended from an operating system
developed at Bell Labs in the 1970s.\footnote{Brian W. Kernighan and Rob
  Pike, \emph{The UNIX Programming Environment} (Englewood Cliffs, New
  Jersey: Prentice-Hall, 1984), vii.} Its original headline feature was
the ability to provide robust multi-user support in a way that clearly
delineated user ownership of files to prevent conflicts.\footnote{Kernighan
  and Pike, \emph{The UNIX Programming Environment}, 1.} It eventually
became a kind of standard: POSIX, the Portable Operating System
Interface.\footnote{Arnold Robbins and Nelson H. F. Beebe, \emph{Classic
  Shell Scripting: Hidden Commands That Unlock the Power of Unix}
  (Sebastapol, California: {``O'Reilly Media, Inc.''} 2005), 1--7.}
Today, MacOS and various forms of Linux and BSD are the most commonly
used UNIXes. They form the backbone of the internet. Most servers run
some form of UNIX.

UNIX OS's share a common structural feature: they consist of a kernel
and a shell. A shell is a textual interface for users to the kernel of
the OS. The shell is a command interpreter. It provides a prompt and a
command language allowing the user to issue written commands to the
kernel and to orchestrate the operation of many programs simultaneously.
Although there are many different shells available to a user, most
implement some substantial portion of the POSIX standard, which means
that a user who learns one shell can usually apply that knowledge to any
UNIX installation. The tools discussed in this presentation are as
standard as possible and should be available for any UNIX system.

On modern desktop and laptop computers with a window-based graphical
user interface, the user accesses the shell through a terminal emulator
program, which provides a place in which to issue commands via the shell
and to receive output from those commands. Commands are issued to the
shell as lines of plain text. This constitutes the Command Line
Interface (CLI). We will be extracting information from pdf files and
manipulating it at the command line using the tools
\VERB|\ExtensionTok{pdfgrep}|, \VERB|\FunctionTok{grep}|,
\VERB|\FunctionTok{awk}|, and \VERB|\ExtensionTok{lualatex}|.\footnote{Other
  forms of \LaTeX, such as \VERB|\ExtensionTok{pdflatex}| and
  \VERB|\ExtensionTok{xelatex}| will serve just as well.} Some
information about how to install \VERB|\ExtensionTok{pdfgrep}| via
common package managers and \VERB|\ExtensionTok{lualatex}| as part of a
\TeX distribution is provided in the acommpanying repository. The tools
\VERB|\FunctionTok{grep}| and \VERB|\FunctionTok{awk}| are already
included in any POSIX compliant environment. Any standard shell may be
used. My examples will use \VERB|\FunctionTok{zsh}|, the default in
MacOS.

One important feature of UNIX tools is their composability. They can be
chained together into a pipeline to achieve a series of transformations
producing the desired result. Textual information flows through UNIX
commands like water flows through a pipe.

\subsection{Key Concepts: PDF Page Labels and Regular
Expressions}\label{key-concepts-pdf-page-labels-and-regular-expressions}

The heart of this workflow turns on two key concepts: pdf page labels
and regular expressions. Such regexes, as they are called will not
receive their own tutorial here, but simple examples in the
demonstration portion will illustrate how they are used. Resources for
further study are available in the associated github repository. In
essence, a regex is a plain text string that represents a pattern. A
regex engine evaluates that string and finds all the strings in the
source document that match the pattern. As originally conceived, regexes
are used by a tool such as \VERB|\FunctionTok{grep}| to search one or
more plain text files. The tool \VERB|\ExtensionTok{pdfgrep}| is a free
and open-source variant of \VERB|\FunctionTok{grep}| that makes it
possible to search a pdf file. Like \VERB|\FunctionTok{grep}| the
\VERB|\ExtensionTok{pdfgrep}| tool is given a regex and one or more
source files and outputs all the matches, along with useful context for
the match: for instance, the filename of the source file in which the
match occurs and the pdf page label of the page on which the match
occurs.

Page labels are a form of metadata in a pdf file that are displayed by
most pdf viewing software, such as MacOS
\VERB|\ExtensionTok{Preview.app}| or KDE \VERB|\ExtensionTok{Okular}|.
They usually indicate a digital text's logical page number corresponding
to its printed original. As in a printed text, cover pages, frontmatter,
body, and backmatter can have distinct pagination. Thus, frontmatter
might be paginated with lowercase roman numerals, while the body might
have arabic numerals. As a result, a given page might, in absolute
terms, be the seventh page in a pdf document, but be labeled with the
number 2 because it has been preceded by a cover page and pages i-iv of
frontmatter. The PDF Association describes these labels as ``an optional
descriptive label of a page that is commonly presented on-screen. This
is in contrast to the integer page index used internally in PDF
files.''\footnote{PDF Association, {``Glossary of PDF Terms''}
  (\url{https://pdfa.org/glossary-of-pdf-terms/\#p}, 2025).} Such labels
are useful for working with a digital version in concert with its
printed \emph{Vorlage}.

Correspondence to the printed original is key to this workflow. If the
labels of the scanned pdf correspond correctly to the printed original,
the list of matches produced by \VERB|\ExtensionTok{pdfgrep}| can easily
be looked up in either the digital or printed version. Moreover, PDF
viewer software typically provides a keyboard shortcut to jump directly
to a specified page label, a feature that is important when scanned
files run to hundreds of pages and lack other forms of navigable
structure.

Unfortunately, a dumb scan of the printed original needs the page labels
added, and most free viewers provide limited functionality, or none at
all, for editing page label metadata or page order. Hence the first step
in the workflow is to repaginate using \LaTeX. We turn now to the
workflow.

\section{Workflow}\label{workflow}

\subsection{Pre-requisite steps}\label{pre-requisite-steps}

We take for granted here that you already have one or more ocr'ed pdf
files that meet the following requirements

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  scanned at 300 dpi or better;
\item
  OCR'ed using a high quality OCR engine (Abbyy recommended);
\item
  and single page: each page of the hard-copy corresponds to a single
  page in the pdf.
\end{enumerate}

\subsection{Phase 1: Repaginate}\label{phase-1-repaginate}

We will use the accompanying file \VERB|\NormalTok{repaginate.tex}|. We
start with the scanned pdf's that need repagination in the same
directory as \VERB|\NormalTok{repaginate.tex}|. We then make any
necessary changes to the \VERB|\NormalTok{repaginate.tex}| file.

If the original file is Beck's translation of the \emph{Hymns on the
Church} with the filename \VERB|\NormalTok{beck\_1960.pdf}|, then we
first examine the scan to determine the absolute page numbers of each
section requiring distinct pagination:

\begin{itemize}
\tightlist
\item
  1-6 should be numbered i-vi
\item
  7-end should be numbered 7-146
\end{itemize}

In \VERB|\NormalTok{repaginate.tex}| we comment out lines 38-39 because
we don't need any cover pages. Then, we change line 54 to include pages
1-6, and the filename to \VERB|\NormalTok{sources/beck\_1960.pdf}|. We
change line 57 to include the rest of the pages 7-end using the string
\VERB|\NormalTok{7{-}}|. Once again we must update the filename to
\VERB|\NormalTok{sources/beck\_1960.pdf}|. This will compile a new pdf
with the specified pages, using the specified page labels.

The final compilation results from the following command:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{lualatex}\NormalTok{ repaginate.tex}
\end{Highlighting}
\end{Shaded}

The result should be a pdf file with the correct page labels.

At this point the file is ready to use for searches. One can build up
several such pdfs in the same way, place them in a single directory, and
search them all at once. For this demonstration we are also going to
establish a file naming convention that will help produce a useful
report at the end. The convention is convenient, but arbitrary, and is
necessary only if you want to use my \VERB|\FunctionTok{awk}| script
without modification. You are free to re-write it to follow some other
convention. The included \VERB|\FunctionTok{awk}| script is designed to
use pdfs with filenames that begin with an abbreviation designating the
collection, followed by a space, followed by any other text. For
example, if one has the \emph{Hymns on the Church}, the \emph{Hymns on
Faith}, and the \emph{Metrical Discourses on Faith}, the filenames for
each would be:
\VERB|\NormalTok{HdE }\DataTypeTok{\textless{}}\KeywordTok{whatever}\DataTypeTok{\textgreater{}}\NormalTok{.pdf}|
and
\VERB|\NormalTok{HdF }\DataTypeTok{\textless{}}\KeywordTok{whatever}\DataTypeTok{\textgreater{}}\NormalTok{.pdf}|
and
\VERB|\NormalTok{SdF }\DataTypeTok{\textless{}}\KeywordTok{whatever}\DataTypeTok{\textgreater{}}\NormalTok{.pdf}|.

\subsection{Phase 2: Explore}\label{phase-2-explore}

Following the UNIX philosophy, we first compose small searches
interactively at the command line. Once one has worked out the pieces,
they can be put together into a single pipeline.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  \textbf{Getting a list of matches from a single text}
\end{enumerate}

This basic search shows how regular expressions can be useful for
capturing text with and without diacriticals, and for capturing
compounds:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pdfgrep} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Ss]ch[aä]tz\textquotesingle{}}\NormalTok{ HdE}\DataTypeTok{\textbackslash{} }\NormalTok{German.pdf }\AttributeTok{{-}H} \AttributeTok{{-}{-}page{-}number}\OperatorTok{=}\NormalTok{label}
\end{Highlighting}
\end{Shaded}

Using the \VERB|\ExtensionTok{{-}H}| option forces the filename to be
output when there is only a single text being searched.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Counting the number of matches}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pdfgrep} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Ss]ch[aä]tz\textquotesingle{}}\NormalTok{ HdE}\DataTypeTok{\textbackslash{} }\NormalTok{German.pdf }\AttributeTok{{-}H} \AttributeTok{{-}c}
\end{Highlighting}
\end{Shaded}

Character classes inside the square brackets will match any of those
characters.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Quickly eyeballing the number of matches across different
  texts}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{pdfgrep} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Ss]ch[aä]tz\textquotesingle{}}\NormalTok{ HdE}\DataTypeTok{\textbackslash{} }\NormalTok{German.pdf HdV}\DataTypeTok{\textbackslash{} }\NormalTok{German.pdf }\AttributeTok{{-}c}
\end{Highlighting}
\end{Shaded}

The \texttt{-c} flag is used to count the number of matches, per file.
The result shows that although the HdV and the HdE are comparable in
line count, HdE seems to use the language of treasure more frequently.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Dealing with lower quality OCR and spelling variation}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{pdfgrep} \PreprocessorTok{*}\NormalTok{.pdf }\AttributeTok{{-}e} \StringTok{\textquotesingle{}G[aei]hen(n)?a\textquotesingle{}} \AttributeTok{{-}{-}page{-}number}\OperatorTok{=}\NormalTok{label }\AttributeTok{{-}H}
\end{Highlighting}
\end{Shaded}

Here we search for the term ``Gehenna'' all the pdfs in the current
directory. The expression \VERB|\ExtensionTok{*.pdf}| is an example of a
glob. The shell expands the star character to a string of any length,
which means that our search will match all the filenames in the
directory that end with the extension ``pdf.'' The proper name Gehenna
admits of more than one spelling, sometimes due to poor quality OCR. But
the same feature is also useful for searching in multiple languages at
once.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Searching in multiple languages simultaneously}
\end{enumerate}

Another way to do that appears in the following code snippet:

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pdfgrep} \AttributeTok{{-}i} \AttributeTok{{-}e} \StringTok{\textquotesingle{}([Kk]ingdom)|(Königtum)|([Rr]eich)\textquotesingle{}} \AttributeTok{{-}{-}page{-}number}\OperatorTok{=}\NormalTok{label }\AttributeTok{{-}H} \PreprocessorTok{*}\NormalTok{.pdf}
\end{Highlighting}
\end{Shaded}

The pipe character in a regular expression serves as a logical
\VERB|\ExtensionTok{OR}|. This permits us to search for the concept of a
kingdom in multiple languages. If we had, for instance, Leloir's French
translation of Ephrem's \emph{Diatessaron} commentary, we could add the
French word for kingdom to the regex, using the pipe character. To make
sure that the logical \VERB|\ExtensionTok{OR}| separates whole words and
not single characters, we group them using round parentheses.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  \textbf{Lookbehinds and pipelines when dealing with many false
  positives}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ExtensionTok{pdfgrep}  \AttributeTok{{-}P} \StringTok{\textquotesingle{}(?\textless{}![Zz]u )Ende(?! des [a{-}z])\textquotesingle{}} 
\ExtensionTok{{-}{-}page{-}number=label} \AttributeTok{{-}H} \AttributeTok{{-}C}\NormalTok{ 3 }
\ExtensionTok{{-}{-}color=always} \PreprocessorTok{*}\NormalTok{.pdf }\KeywordTok{|} \FunctionTok{grep} \AttributeTok{{-}v} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Zz]u\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

In this example, we find instances of the word \emph{Ende} but which are
not preceded by the word \emph{Zu}, because the phrase \emph{Zu Ende} is
very common. It is Beck's usual translation of \emph{šlem}, a scribal
note in the mss. typical at the end of a \emph{madrāšâ} or collection of
\emph{madrāšê}. Being a scribal note, it is of no interest for
understanding Ephrem's word usage, so we exclude it in the present
inquiry.

Doing this requires the use of a negative lookbehind, which is not a
feature of basic or extended regular expressions.\footnote{{``Advanced
  Grep Topics''}
  (\url{https://caspar.bgsu.edu/~courses/Stats/Labs/Handouts/grepadvanced.htm},
  2023); Michael Fitzgerald, \emph{Introducing Regular Expressions}
  (Sebastapol, California: {``O'Reilly Media, Inc.''} 2012), 78.} Thus
we must invoke \VERB|\ExtensionTok{pdfgrep}| with perl compatible
regular expressions, a more sophisticated tool. We invoke it via the
\texttt{-P} flag. The negative lookbehind itself is
\texttt{(?\textless{}!\ .\ .\ .)} and the negative lookahead is
\texttt{(?!\ .\ .\ .)}. This excludes matches in which the word
\emph{Ende} is preceded or followed by the most common patterns we wish
to exclude.

But there's still a problem. OCR'ed texts often have inconsistent
whitespace between words, and thus there are still some passages in
which the undesired matches are included due to variable whitespace.
Lookbehinds and Lookaheads require fixed length strings, so we cannot
account for the variability that way. Thus we pipe the results into a
second invocation of \texttt{grep} to filter out any instances of
\emph{Zu} remaining. The inversion flag \texttt{-v} accomplishes this
nicely. The result is not perfect, but it excludes a sufficient number
of false positives that any remaining ones can be discovered manually.

Here's a version that omits the \texttt{-C} flag, which is not useful
when using the command in a pipeline to produce a report.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  \textbf{Modified Version}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pdfgrep}  \AttributeTok{{-}P} \StringTok{\textquotesingle{}(?\textless{}![Zz]u )Ende(?! des [a{-}z])\textquotesingle{}} 
\ExtensionTok{{-}{-}page{-}number=label} \AttributeTok{{-}H}  
\ExtensionTok{{-}{-}color=always} \PreprocessorTok{*}\NormalTok{.pdf }\KeywordTok{|} \FunctionTok{grep} \AttributeTok{{-}v} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Zz]u\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\subsection{Phase 3: Report}\label{phase-3-report}

By default, these commands simply print the information they produce to
your screen, referred to as standard output: \texttt{stdout}. Once
you've built up a number of small searches that can easily be recalled
using your shell's history function, you may wish to save the results to
a file so that you can use them in the future. The easiest way to do
this is to simple redirect to output stream to a file, using a
redirection operator: \texttt{\textgreater{}}.\footnote{Chet Ramey and
  Brian Fox, \emph{The GNU Bash Reference Manual}, 5.2 ed., 2022, sec.
  3.6.} For example:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{7}
\tightlist
\item
  \textbf{Redirection example}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{pdfgrep} \AttributeTok{{-}e} \StringTok{\textquotesingle{}[Ss]ch[aä]tz\textquotesingle{}}\NormalTok{ HdE}\DataTypeTok{\textbackslash{} }\NormalTok{German.pdf }
\ExtensionTok{{-}H} \AttributeTok{{-}{-}page{-}number}\OperatorTok{=}\NormalTok{label }\OperatorTok{\textgreater{}}\NormalTok{ results.txt}
\end{Highlighting}
\end{Shaded}

The result is a plain text file that matches what is output on screen,
but without any colors. It is possible to output this information into
even more useful formats with the help of another standard UNIX tool:
\texttt{awk}. Like \texttt{grep} \texttt{awk} uses regexes. Its data
model is to loop through all the lines of a file, performing tasks
depending on what it finds. In this case, we can use it to transform the
output of pdfgrep into a csv (comma separated value) file that can be
opened in a spreadsheet application or fed into a data analysis
pipeline.

The repository includes a very simple awk script
(\texttt{grep\_to\_csv.awk}) that parses the output of \texttt{pdfgrep}
and structures it as a csv file:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{BEGIN} \OperatorTok{\{} \BuiltInTok{FS} \OperatorTok{=} \StringTok{":"}\OperatorTok{;} \KeywordTok{print} \StringTok{"text"}\OperatorTok{,} \StringTok{","}\OperatorTok{,} \StringTok{"citation"} \OperatorTok{\}}
\OperatorTok{\{} 
\NormalTok{    cutoff }\OperatorTok{=} \FunctionTok{index}\OperatorTok{(}\DataTypeTok{$1}\OperatorTok{,} \StringTok{" "}\OperatorTok{);}
    \CommentTok{\# print "cutoff index is " cutoff \# for debugging purposes}
    \KeywordTok{print} \FunctionTok{substr}\OperatorTok{(}\DataTypeTok{$1}\OperatorTok{,} \DecValTok{1}\OperatorTok{,}\NormalTok{ cutoff}\OperatorTok{)} \StringTok{","}\OperatorTok{,} \DataTypeTok{$2} 
\OperatorTok{\}} 
\end{Highlighting}
\end{Shaded}

This awk script begins by setting the field separator to \texttt{:},
which always appears when \texttt{grep} is invoked with the \texttt{-H}
flag. Doing this causes the first field or chunk of text on each line to
be the filename in which a given match was found. It then uses
\texttt{awk}'s built in string \texttt{index()} function to determine
where a space occurs in that first field first field and the
\texttt{substr()} (substring) function to extract the first part of the
filename before the space.\footnote{Alfred V. Aho, Brian W. Kernighan,
  and Peter J. Weinberger, \emph{The AWK Programming Language}
  (Addison-Wesley, 1988), 41--43. This is the original manual, but still
  relevant. \texttt{Awk} is included in the POSIX standard, but the most
  up to date version can be found at
  \href{https://github.com/onetrueawk/awk}{onetrueawk}.} Per our naming
convention, this substring constitutes the abbreviation of the work in
question: \emph{HdE} or \emph{SdF}, for example. And the \texttt{print}
instruction prints that abbreviation, followed by a comma, followed by
the page number produced by \texttt{pdfgrep} for each match. The result
is a simple csv file.

Here is an example of the whole pipeline in action:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{8}
\tightlist
\item
  \textbf{Report pipeline}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
 \ExtensionTok{pdfgrep} \PreprocessorTok{*}\NormalTok{.pdf }\AttributeTok{{-}e} \StringTok{\textquotesingle{}G[aei]hen(n)?a\textquotesingle{}}
  \ExtensionTok{{-}{-}page{-}number=label}  \KeywordTok{|} \FunctionTok{awk} \AttributeTok{{-}f}\NormalTok{ grep\_to\_csv.awk }\OperatorTok{\textgreater{}}\NormalTok{ gehenna.csv}
\end{Highlighting}
\end{Shaded}

This produces a file called \texttt{gehenna.csv} containing all the
instances where the term ``Gehenna'' appears in all pdf files in the
directory.

\section{Summary and Limitations}\label{summary-and-limitations}

UNIX command line tools, when put together, constitute a powerful,
flexible, and free way to construct a searchable corpus of pdfs from
physical originals, explore the corpus, and produce usable reports to
show approximate information about the location, frequency, and
distribution of vocabulary of interest.

Because the OCR on which it relies is imperfect, this workflow requires
care and attention to use in a responsible way. It will produce false
positives. It will miss some (possibly) important passages in which the
vocabulary of interest appears. Matches should always be humanly
verified. And of course, a list of matches functions best as a guide for
reading the original sources, which is the goal of this workflow. It
points out the likely places for further study, much as a print or
digital concordance does. The difference between this approach and a
concordance is that you are able to construct the concordance to suit
your exact requirements, and that you can update it over time as you
improve and curate your corpus of scanned texts.

\clearpage

\section*{Bibliography}\label{bibliography}
\addcontentsline{toc}{section}{Bibliography}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-grep2023b}
{``Advanced Grep Topics.''}
\url{https://caspar.bgsu.edu/~courses/Stats/Labs/Handouts/grepadvanced.htm},
2023.

\bibitem[\citeproctext]{ref-aho1988a}
Aho, Alfred V., Brian W. Kernighan, and Peter J. Weinberger. \emph{The
AWK Programming Language}. Addison-Wesley, 1988.

\bibitem[\citeproctext]{ref-brock2025}
Brock, Sebastian. {``Bibliographical Handouts by Brock.''}
\url{https://syri.ac/brock}, 2025.

\bibitem[\citeproctext]{ref-fitzgerald2012}
Fitzgerald, Michael. \emph{Introducing Regular Expressions}. Sebastapol,
California: {``O'Reilly Media, Inc.''} 2012.

\bibitem[\citeproctext]{ref-kernighan1984}
Kernighan, Brian W., and Rob Pike. \emph{The UNIX Programming
Environment}. Englewood Cliffs, New Jersey: Prentice-Hall, 1984.

\bibitem[\citeproctext]{ref-pdfassociation2025}
PDF Association. {``Glossary of PDF Terms.''}
\url{https://pdfa.org/glossary-of-pdf-terms/\#p}, 2025.

\bibitem[\citeproctext]{ref-ramey2022}
Ramey, Chet, and Brian Fox. \emph{The GNU Bash Reference Manual}. 5.2
ed., 2022.

\bibitem[\citeproctext]{ref-robbins2005}
Robbins, Arnold, and Nelson H. F. Beebe. \emph{Classic Shell Scripting:
Hidden Commands That Unlock the Power of Unix}. Sebastapol, California:
{``O'Reilly Media, Inc.''} 2005.

\end{CSLReferences}

\end{document}
